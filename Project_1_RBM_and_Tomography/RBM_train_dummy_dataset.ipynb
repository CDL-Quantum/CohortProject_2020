{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T09:34:12.950481Z",
     "start_time": "2020-06-03T09:34:12.945954Z"
    }
   },
   "source": [
    "# Training an RBM on dummy data\n",
    "\n",
    "In this notebook, we'll go through some basics on how to train an RBM. The input data we will use is going to be synthetic.\n",
    "\n",
    "First, import the things we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T20:47:45.378055Z",
     "start_time": "2020-06-25T20:47:45.031933Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from RBM_helper import RBM\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of training steps (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T20:47:45.610013Z",
     "start_time": "2020-06-25T20:47:45.605356Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the dummy data. The dummy data will contain the strings `[1,0,1,0]` and `[0,1,0,1]` with equal probability. After we train the RBM on this data, the RBM should only reproduce these two strings with equal probability (if the training went well!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T20:47:46.228459Z",
     "start_time": "2020-06-25T20:47:46.221505Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[1,0,1,0]]*1000 + [[0,1,0,1]]*1000)\n",
    "data = torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the RBM. We choose 3 visible units (because the input data is 4 dimensional) and 4 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T20:47:46.963680Z",
     "start_time": "2020-06-25T20:47:46.957895Z"
    }
   },
   "outputs": [],
   "source": [
    "vis = len(data[0]) #input dimension\n",
    "\n",
    "n_vis = vis\n",
    "n_hid = vis # set the number of hidden units to the number of visible units for now\n",
    "\n",
    "rbm = RBM(n_vis, n_hid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T20:47:54.088346Z",
     "start_time": "2020-06-25T20:47:47.886176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  50\n",
      "Epoch:  100\n",
      "Epoch:  150\n",
      "Epoch:  200\n",
      "Epoch:  250\n",
      "Epoch:  300\n",
      "Epoch:  350\n",
      "Epoch:  400\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,epochs+1):\n",
    "    rbm.train(data)\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch: \",epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the RBM learn from the data at all? Let's find out. Draw 10 samples from RBM and print them. If the training was successful, the samples should be reminiscent of the input dummy data. You can also increase the number of samples and count how often each sample appears. They should be equally likely, because they are also equally likely in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T20:47:54.146417Z",
     "start_time": "2020-06-25T20:47:54.140069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "k = 20 \n",
    "init_state = torch.zeros(num_samples, n_vis)\n",
    "rbm_samples = rbm.draw_samples(k, init_state)\n",
    "print(rbm_samples.detach().numpy())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! It looks like the RBM learned the distribution of the input data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
